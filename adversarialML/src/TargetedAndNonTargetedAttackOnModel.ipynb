{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The notebook walks you through generating adversarial images through targeted and non-targeted attacks. And it also introduces you to a very basic defense technique (binary thresholding).The goal of this question is to construct adversarial examples to attack a machine learning system (digit recognition). You will also create a simple defense and check whether the defense works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "bQPR1DJzW9pT",
        "outputId": "71a74e0c-7aa2-454e-fb5e-568cbbbb7533"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7b5f09906415>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Imports.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist_loader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmnist_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'network'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Imports.\n",
        "import random\n",
        "import network.network as Network\n",
        "import network.mnist_loader as mnist_loader\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Set the random seed. DO NOT CHANGE THIS!\n",
        "seedVal = 41\n",
        "random.seed(seedVal)\n",
        "np.random.seed(seedVal)\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYkOngVrW9pU"
      },
      "source": [
        "Use a pre-trained network. It has been saved as a pickle file. Load the model, and continue.\n",
        "The network has only one hidden layer of 30 units, 784 input units (MNIST images are $ 28 \\times 28 = 784 $ pixels large), and 10 output units. All the activations are sigmoidal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ikvBp9gW9pV"
      },
      "outputs": [],
      "source": [
        "# Load the pre-trained model.\n",
        "with open('network/trained_network.pkl', 'rb') as f:\n",
        "    u = pickle._Unpickler(f)\n",
        "    u.encoding = 'latin1'\n",
        "    net = u.load()\n",
        "\n",
        "# Helpful function to load the MNIST data.\n",
        "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fybHt8KW9pV"
      },
      "source": [
        "The neural network is pretrained, so it should already be set up to predict characters. Run `predict(n)` to evaluate the $ n^{th} $ digit in the test set using the network. You should see that even this relatively simple network works really well (~97% accuracy). The output of the network is a one-hot vector indicating the network's predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUVaYBuOW9pV"
      },
      "outputs": [],
      "source": [
        "def predict(n):\n",
        "    # Get the data from the test set\n",
        "    x = test_data[n][0]\n",
        "    \n",
        "    # Print the prediction of the network\n",
        "    print('Network output: \\n' + str(np.round(net.feedforward(x), 2)) + '\\n')\n",
        "    print('Network prediction: ' + str(np.argmax(net.feedforward(x))) + '\\n')\n",
        "    print('Actual image: ')\n",
        "    \n",
        "    # Draw the image\n",
        "    plt.imshow(x.reshape((28,28)), cmap='Greys')\n",
        "\n",
        "# Replace the argument with any number between 0 and 9999\n",
        "predict(8384)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4TSDblIW9pW"
      },
      "source": [
        "To actually generate adversarial examples we solve a minimization problem. We do this by setting a \"goal\" label called $ \\vec y_{goal} $ (for instance, if we wanted the network to think the adversarial image is an 8, then we would choose $ \\vec y_{goal} $ to be a one-hot vector with the eighth entry being 1). Now we define a cost function:\n",
        "\n",
        "$$ C = \\frac{1}{2} \\|\\vec y_{goal} - \\hat y(\\vec x)\\|^2_2 $$\n",
        "\n",
        "where $ \\| \\cdot \\|^2_2 $ is the squared Euclidean norm and $ \\hat y $ is the network's output. It is a function of $ \\vec x $, the input image to the network, so we write $ \\hat y(\\vec x) $. Our goal is to find an $ \\vec x $ such that $ C $ is minimized. Hopefully this makes sense, because if we find an image $ \\vec x $ that minimizes $ C $ then that means the output of the network when given $ \\vec x $ is close to our desired output, $ \\vec y_{goal} $. So in full mathy language, our optimization problem is:\n",
        "\n",
        "$$ \\arg \\min_{\\vec x} C(\\vec x) $$\n",
        "\n",
        "that is, find the $ \\vec x $ that minimizes the cost $ C $.\n",
        "\n",
        "To actually do this we can do gradient descent on $ C $. Start with an initially random vector $ \\vec x $ and take steps (changing $ \\vec x $) gradually in the direction opposite of the gradient $ \\nabla_x C $. To actually get these derivatives we can perform backpropagation on the network. In contrast to training a network, where we perform gradient descent on the weights and biases, when we create adversarial examples we hold the weights and biases constant (because we don't want to change the network!), and change the inputs to our network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0vL9KrFW9pX"
      },
      "source": [
        "Helper functions to evaluate the non-linearity and it's derivative:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cen1UWo_W9pX"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    \"\"\"The sigmoid function.\"\"\"\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "                                                                                                                                                                                \n",
        "def sigmoid_prime(z):\n",
        "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
        "    return sigmoid(z)*(1-sigmoid(z))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXdbMZvtW9pX"
      },
      "source": [
        "Also, a function to find the gradient derivatives of the cost function, $ \\nabla_x C $ with respect to the input $ \\vec x $, with a goal label of $ \\vec y_{goal} $. (Don't worry too much about the implementation, just know it calculates derivatives)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5ny8hfrW9pY"
      },
      "outputs": [],
      "source": [
        "def input_derivative(net, x, y):\n",
        "    \"\"\" Calculate derivatives wrt the inputs\"\"\"\n",
        "    nabla_b = [np.zeros(b.shape) for b in net.biases]\n",
        "    nabla_w = [np.zeros(w.shape) for w in net.weights]\n",
        "    \n",
        "    # feedforward\n",
        "    activation = x\n",
        "    activations = [x] # list to store all the activations, layer by layer\n",
        "    zs = [] # list to store all the z vectors, layer by layer\n",
        "    for b, w in zip(net.biases, net.weights):\n",
        "        z = np.dot(w, activation)+b\n",
        "        zs.append(z)\n",
        "        activation = sigmoid(z)\n",
        "        activations.append(activation)\n",
        "        \n",
        "    # backward pass\n",
        "    delta = net.cost_derivative(activations[-1], y) * \\\n",
        "        sigmoid_prime(zs[-1])\n",
        "    nabla_b[-1] = delta\n",
        "    nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "\n",
        "    for l in range(2, net.num_layers):\n",
        "        z = zs[-l]\n",
        "        sp = sigmoid_prime(z)\n",
        "        delta = np.dot(net.weights[-l+1].transpose(), delta) * sp\n",
        "        nabla_b[-l] = delta\n",
        "        nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "        \n",
        "    # Return derivatives WRT to input\n",
        "    return net.weights[0].T.dot(delta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRggUca-W9pY"
      },
      "source": [
        "The actual function that generates adversarial examples and a wrapper function:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaFQr3T6W9pY"
      },
      "source": [
        "## (a) Non Targeted Attack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XeUGXh1W9pY"
      },
      "outputs": [],
      "source": [
        "def nonTargetedAdversarial(net, n, steps, eta):\n",
        "    \"\"\"\n",
        "    net : network object\n",
        "        neural network instance to use\n",
        "    n : integer\n",
        "        our goal label (just an int, the function transforms it into a one-hot vector)\n",
        "    steps : integer\n",
        "        number of steps for gradient descent\n",
        "    eta : float\n",
        "        step size for gradient descent\n",
        "    \"\"\"\n",
        "    \n",
        "    ####### Enter your code below #######\n",
        "    \n",
        "    # Set the goal output\n",
        "    \n",
        "    goal[n] = 1\n",
        "\n",
        "    # Create a random image to initialize gradient descent with\n",
        "    \n",
        "    # Gradient descent on the input\n",
        "    for i in range(None):\n",
        "        # Calculate the derivative\n",
        "    \n",
        "        # The GD update on x\n",
        "            \n",
        "    return x\n",
        "\n",
        "\n",
        "# Wrapper function \n",
        "def generate(n):\n",
        "    \"\"\"\n",
        "    n : integer\n",
        "        goal label (not a one hot vector)\n",
        "    \"\"\"\n",
        "    \n",
        "    ####### Enter your code below #######\n",
        "    \n",
        "    # Find the vector x with the above function that you just wrote.\n",
        "    \n",
        "    # Pass the generated image (vector) to the neural network. Perform a forward pass, and get the prediction.\n",
        "    \n",
        "    print('Network Output: \\n' + str(x) + '\\n')\n",
        "    \n",
        "    print('Network Prediction: ' + str(np.argmax(x)) + '\\n')\n",
        "    \n",
        "    print('Adversarial Example: ')\n",
        "    \n",
        "    plt.imshow(a.reshape(28,28), cmap='Greys')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2JpP-ELW9pZ"
      },
      "source": [
        "Now let's generate some adversarial examples! Use the function provided to mess around with the neural network. (For some inputs gradient descent doesn't always converge; 0 and 5 seem to work pretty well though. I suspect convergence is very highly dependent on our choice of random initial $ \\vec x $. We'll see later in the notebook if we force the adversarial example to \"look like\" a handwritten digit, convergence is much more likely. In a sense we will be adding regularization to our generation process)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVlyGE1PW9pZ"
      },
      "outputs": [],
      "source": [
        "generate(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYo5Sr9lW9pZ"
      },
      "source": [
        "## (b) Targeted Attack(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhIUU4AeW9pZ"
      },
      "source": [
        "Sweet! We've just managed to create an image that looks utterly meaningless to a human, but the neural network thinks is a '5' with very high certainty. We can actually take this a bit further. Let's generate an image that looks like one number, but the neural network is certain is another. To do this we will modify our cost function a bit. Instead of just optimizing the input image, $ \\vec x $, to get a desired output label, we'll also optimize the input to look like a certain image, $ \\vec x_{target} $, at the same time. Our new cost function will be\n",
        "\n",
        "$$ C = \\|\\vec y_{goal} - y_{hat}(\\vec x)\\|^2_2 + \\lambda \\|\\vec x - \\vec x_{target}\\|^2_2 $$\n",
        "\n",
        "The added term tells us the distance from our $ \\vec x $ and some $ \\vec x_{target} $ (which is the image we want our adversarial example to look like). Because we want to minimize $ C $, we also want to minimize the distance between our adversarial example and this image. The $ \\lambda $ is hyperparameter that we can tune; it determines which is more important: optimizing for the desired output or optimizing for an image that looks like $ \\vec x_{target} $.\n",
        "\n",
        "If you are familiar with ridge regularization, the above cost function might look suspiciously like the ridge regression cost function. In fact, we can view this generation method as giving our model a prior, centered on our target image.\n",
        "\n",
        "Here is a function that implements optimizing the modified cost function, called `sneaky_adversarial` (because it is very sneaky). Note that the only difference between this function and `adversarial` is an additional term on the gradient descent update for the regularization term:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKx0vstWW9pZ"
      },
      "outputs": [],
      "source": [
        "# Probably include a question on why not generate an image of 5, instead of having to find the vector x using \n",
        "# gradient based optimization methods. (as in the above case)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wq6qz4XsW9pa"
      },
      "outputs": [],
      "source": [
        "def targetedAdversarial(net, n, x_target, steps, eta, lam=.05):\n",
        "    \"\"\"\n",
        "    net : network object\n",
        "        neural network instance to use\n",
        "    n : integer\n",
        "        our goal label (just an int, the function transforms it into a one-hot vector)\n",
        "    x_target : numpy vector\n",
        "        our goal image for the adversarial example\n",
        "    steps : integer\n",
        "        number of steps for gradient descent\n",
        "    eta : float\n",
        "        step size for gradient descent\n",
        "    lam : float\n",
        "        lambda, our regularization parameter. Default is .05\n",
        "    \"\"\"\n",
        "    \n",
        "    # Set the goal output\n",
        "    \n",
        "    # Create a random image to initialize gradient descent with\n",
        "\n",
        "    # Gradient descent on the input\n",
        "    for i in range(None):\n",
        "        \n",
        "        # Calculate the derivative\n",
        "        \n",
        "        # The GD update on x, with an added penalty to the cost function\n",
        "\n",
        "    return x\n",
        "\n",
        "# Wrapper function\n",
        "def generate_advSample(n, m):\n",
        "    \"\"\"\n",
        "    n: int 0-9, the target number to match\n",
        "    m: index of example image to use (from the test set)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Find random instance of m in test set\n",
        "    idx = np.random.randint(0,8000)\n",
        "    while test_data[idx][1] != m:\n",
        "        idx += 1\n",
        "    \n",
        "    # Hardcode the parameters for the wrapper function\n",
        "    a = targetedAdversarial(net, n, test_data[idx][0], 100, 1)\n",
        "    x = np.round(net.feedforward(a), 2)\n",
        "    \n",
        "    print('\\nWhat we want our adversarial example to look like: ')\n",
        "    plt.imshow(test_data[idx][0].reshape((28,28)), cmap='Greys')\n",
        "    plt.show()\n",
        "    \n",
        "    print('\\n')\n",
        "    \n",
        "    print('Adversarial Example: ')\n",
        "    \n",
        "    plt.imshow(a.reshape(28,28), cmap='Greys')\n",
        "    plt.show()\n",
        "    \n",
        "    print('Network Prediction: ' + str(np.argmax(x)) + '\\n')\n",
        "    \n",
        "    print('Network Output: \\n' + str(x) + '\\n')\n",
        "    \n",
        "    return a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yo2zc-26W9pa"
      },
      "source": [
        "Play around with this function to make \"sneaky\" adversarial examples! (Again, some numbers converge better than others... try 0, 2, 3, 5, 6, or 8 as a target label. 1, 4, 7, and 9 still don't work as well... no idea why... We get more numbers that converge because we've added regularization term to our cost function. Perhaps changing $ \\lambda $ will get more to converge?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uCRu8SxW9pa"
      },
      "outputs": [],
      "source": [
        "# generate_advSample(target label, target digit)\n",
        "adv_ex = generate_advSample(8, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkyxkMRnW9pa"
      },
      "source": [
        "## (c) Protection against adversarial attacks"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "yCp3U21jW9pa"
      },
      "source": [
        "Awesome! We’ve just created images that trick neural networks. The next question we could ask is whether or not we could protect against these kinds of attacks. If you look closely at the original images and the adversarial examples you’ll see that the adversarial examples have some sort of grey tinged background."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci4nFvF_W9pa"
      },
      "source": [
        "So how could we protect against these adversarial attacks? One very simple way would be to use binary thresholding. Set a pixel as completely black or completely white depending on a threshold. This should remove the \"noise\" that's always present in the adversarial images. Let's see if it works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWm_i1FJW9pa"
      },
      "outputs": [],
      "source": [
        "def simple_defense(n, m):\n",
        "    \"\"\"\n",
        "    n: int 0-9, the target number to match\n",
        "    m: index of example image to use (from the test set)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Generate an adversarial sample.\n",
        "    x = generate_advSample(n, m)\n",
        "    \n",
        "    # Perform binary thresholding on the generated sample. You can choose the threshold as 0.5.\n",
        "      \n",
        "    print(\"With binary thresholding: \")\n",
        "    \n",
        "    # Plot a grayscale image of the binarized generated sample.\n",
        "\n",
        "    # Print the network's predictions.\n",
        "    print(\"Prediction with binary thresholding: \" + None + '\\n')\n",
        "    \n",
        "    # The output of the network.\n",
        "    print(\"Network output: \")\n",
        "    print( None )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Opc73ge8W9pb"
      },
      "outputs": [],
      "source": [
        "# binary_thresholding(target digit, actual digit)\n",
        "simple_defense(0, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ba1oIN9aW9pb"
      },
      "source": [
        "Looks like it works pretty well! However, note that most adversarial attacks, especially on convolutional neural networks trained on massive full color image sets such as imagenet, can't be defended against by a simple binary threshold."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
