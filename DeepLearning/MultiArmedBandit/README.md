<p align="center">
  <img src="./img/mab/mab.png" alt="MAB Image" width="400" height="300"/>
</p>

# Multi-Armed Bandit (MAB)

The Multi-Armed Bandit (MAB) problem is a fundamental challenge in decision-making under uncertainty. It mirrors a gambler facing multiple slot machines with unknown payout rates, illustrating the delicate balance between exploration and exploitation. Rooted in probability and statistics, MAB problems have broad applications, from refining online algorithms to guiding strategic decisions in business and healthcare, where adapting to evolving environments with limited information is crucial.

## Overview

This research conducts an in-depth empirical analysis of Frequentist and Bayesian strategies for addressing the Stochastic Multi-Armed Bandit problem. It delves into the mathematical complexities of decision-making under uncertainty, employing advanced models and simulations across various scenarios. The study provides valuable insights into algorithm performance, focusing on reward maximization, best-arm identification, and computational efficiency, offering a nuanced understanding of the exploration-exploitation trade-off.

## Applications

The study underscores the practical relevance of these strategies in domains such as online advertising, economics, and healthcare. Through comprehensive comparisons, it paves the way for optimizing decision-making algorithms to tackle real-world challenges.

For code and results, refer to [main.ipynb](link-to-your-main.ipynb).
</br>
- [Report](./MAB-Report.pdf)
