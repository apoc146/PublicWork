# Adversarial Machine Learning
<section id="readme-top"></section>

<!-- ![banner]() -->

<!-- ![badge]()
![badge]()
[![license](https://img.shields.io/github/license/:user:repo.svg)](LICENSE)
[![standard-readme compliant](https://img.shields.io/badge/readme%20style-standard-brightgreen.svg?style=flat-square)](https://github.com/RichardLitt/standard-readme) -->

<!-- This is an example file with maximal choices selected.

This is a long description. -->
## Description

This repo dives into challenges and methodologies surrounding the security aspects of machine learning models against adversarial attacks.

## Content

- Attacks on the Machine Learning Pipeline: Poisoning attacks, model theft attacks, adversarial examples, recovery of sensitive training data, and physical-world attacks
- Evasion, Poisoning, and Exploratory Attacks
- Types of Defenses: Pre-processing, and robust optimization
- Introduction to Privacy in Machine Learning: Membership inference and model inversion attacks
- Paper Reviews and Case Studies

## Stack
- **Programming Language:**
  - Python
- **Machine Learning & Deep Learning Libraries:**
  - Scikit-learn
  - Pytorch
  - TensorFlow
  - Keras
- **Data Processing & Analysis:**
  - Pandas
  - NumPy
- **Documentation:**
  - LaTeX
- **Adversarial Machine Learning:**
  - Adversarial Machine Learning Techniques
  - Evasion Attacks
  - Poisoning Attacks
  - Defense Strategies
  - Model Robustness
- **Tools:**
  - ART (Adversarial Robustness Toolbox)

## [Papers Reviews Se](#papers-reviews)

## Results

The source code for all projects and analyses can be found in the [`src`](./src) directory. Below are some highlighted results from the work:


### Non Targeted Attack

<div align="center">

![Making a Neural Network misclassify an adversarial image as 'Number 2'](img/nontargetAttack.png)

<em> Figure 1: An illustration of an targeted evasion attack, where a neural network is tricked into misclassifying an adversarial image as 'Number 2'.</em>
</div>

### Targeted Attack

<div align="center">

<img src="img/tarAttack.png" alt="Result 2 Description" width="350px">
<br>
<em>Figure 2:Making a Neural Network misclassify a legible 2 adversarial image as 'Number 8'</em>
</div>
</br>

### Defense Strategies Against Adversarial Attacks

<div align="center">
  <img src="img/defence.png" alt="Binary Thresholding Defense" width="300">
  <br>
  <em>Figure 3: Demonstrating binary thresholding as an effective defense mechanism against adversarial attacks.</em>
</div>
</br>

### Adversarial Example Generation with IBM ART

<div align="center">
  <img src="img/ibmArt1.png" alt="IBM ART Adversarial Examples" width="300">
  <br>
  <em>Figure 4: Adversarial examples created using IBM ART, employing techniques such as FGSM, Basic Iterative Method, Saliency Map Method, and Universal Perturbation.</em>
</div>
</br>

### CNN Performance on Adversarial Examples from IBM ART

<div align="center">
  <img src="img/ibmARTper.png" alt="CNN Performance on Adversarial Examples" width="300">
  <br>
  <em>Figure 5: Evaluating a Convolutional Neural Network's performance in classifying adversarial examples generated by IBM ART.</em>
</div>
</br>

### Exploring Additional Adversarial Attacks with ART

<div align="center">
  <img src="img/img1.png" alt="ART Attacks on Neural Networks" width="300">
  <br>
  <em>Figure 6: Assessing the impact of various Adversarial Robustness Toolbox (ART) attack strategies on neural network performance.</em>
</div>

### Transferability of Adversarial Examples

<div align="center">
  <img src="img/img2.png" alt="Adversarial Example Transferability" width="400">
  <br>
  <em>Figure 7: Investigating the transferability of adversarial examples across different machine learning classifiers.</em>
</div>
</br>

### Generative Adversarial Networks: DCGAN and CGAN

<div align="center">
  <img src="img/gans.png" alt="DCGAN and CGAN Performance" width="600">
  <br>
  <em>Figure 8: Showcasing the capabilities and outcomes of Deep Convolutional GAN (DCGAN) and Conditional GAN (CGAN) in generating adversarial images.</em>
</div>
</br>

## Papers Reviews

1. [Dos and Don'ts of Machine Learning in Computer Security](https://www.usenix.org/system/files/sec22summer_arp.pdf)  
   - Summary: Discusses best practices and pitfalls in utilizing machine learning techniques for enhancing computer security.
   - [Paper Review](./Paper%20Review/Review1.pdf)

2. [6thSense: A Context-aware Sensor-based Attack Detector for Smart Devices](https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/sikder)  
   - Summary: Presents 6thSense, a novel attack detection system for smart devices leveraging context-aware sensor data.
   - [Paper Review](./Paper%20Review/Review2.pdf)

3. [SoK: Security and Privacy in Machine Learning](https://oaklandsok.github.io/papers/papernot2018.pdf)  
   - Summary: Provides a comprehensive survey of the security and privacy challenges in machine learning systems.
   - [Paper Review](./Paper%20Review/Review3.pdf)

4. [DEEPSEC: A Uniform Platform for Security Analysis of Deep Learning Models](https://ieeexplore.ieee.org/document/8835375)  
   - Summary: Introduces DEEPSEC, a platform for analyzing the security aspects of deep learning models.
   - [Paper Review](./Paper%20Review/Review4.pdf)

## License
Not Distributed

<p align="right">(<a href="#readme-top">back to top</a>)</p>

<!-- Not Distributed under the MIT License. See `LICENSE.txt` for more information. -->

<!-- <p align="right">(<a href="#readme-top">back to top</a>)</p> -->

<!-- CONTACT -->
## Contact

[Shivam](https://twitter.com/) - bhat41@purdue.edu

<!-- Project Link: [https://github.com/your_username/repo_name](https://github.com/your_username/repo_name) -->

<!-- <p align="right">(<a href="#readme-top">back to top</a>)</p> -->

<!-- ACKNOWLEDGMENTS -->
## Acknowledgments
* [Course : CS529](https://beerkay.github.io/cs529/content/syllabus/CS529.pdf)
